"""
Knowledge Pipeline - LLM Provider æŠ½è±¡å±¤ä»‹é¢å®šç¾©

æ­¤æ¨¡çµ„å®šç¾© LLM Provider çš„æŠ½è±¡ä»‹é¢èˆ‡è³‡æ–™æ¨¡å‹ï¼Œ
æ”¯æ´å¤šç¨® LLM å¯¦ä½œï¼ˆGemini CLIã€OpenAI APIã€Gemini APIã€Local LLM ç­‰ï¼‰ã€‚

æ¶æ§‹:
    LLMClient (é€šç”¨å…¥å£)
        â””â”€â”€ LLMProvider (Protocol)
            â”œâ”€â”€ GeminiCLIProvider (ç•¶å‰å¯¦ä½œ)
            â”œâ”€â”€ OpenAIProvider (é ç•™)
            â””â”€â”€ LocalLLMProvider (é ç•™)

ä½¿ç”¨ç¯„ä¾‹:
    from src.llm import LLMClient, TranscriptInput
    
    client = LLMClient.from_config({
        "provider": "gemini_cli",
        "project_dir": "/path/to/project",
        "timeout": 300
    })
    
    result = client.analyze(
        input_data=transcript_input,
        prompt_template="crypto_tech"
    )
"""

from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime
from enum import Enum, auto
from pathlib import Path
from typing import Protocol, runtime_checkable


# ============================================================================
# Enum å®šç¾©
# ============================================================================

class ProviderType(str, Enum):
    """æ”¯æ´çš„ LLM Provider é¡å‹"""
    GEMINI_CLI = "gemini_cli"
    OPENAI_API = "openai_api"
    GEMINI_API = "gemini_api"
    LOCAL_LLM = "local_llm"


# ============================================================================
# è¼¸å…¥è¼¸å‡ºè³‡æ–™æ¨¡å‹
# ============================================================================

@dataclass
class TranscriptInput:
    """
    è¼¸å…¥çµ¦ LLM çš„æ¨™æº–åŒ–è½‰éŒ„è³‡æ–™
    
    æ‰€æœ‰ Provider éƒ½ä½¿ç”¨æ­¤çµ±ä¸€æ ¼å¼ä½œç‚ºè¼¸å…¥ã€‚
    
    Attributes:
        channel: YouTube é »é“åç¨±
        title: å½±ç‰‡æ¨™é¡Œ
        content: å®Œæ•´è½‰éŒ„å…§å®¹ï¼ˆç´”æ–‡å­—ï¼‰
        published_at: ç™¼å¸ƒæ—¥æœŸï¼ˆISO 8601 æ ¼å¼å­—ä¸²ï¼‰
        word_count: è½‰éŒ„å­—æ•¸
        file_path: åŸå§‹æª”æ¡ˆè·¯å¾‘
        video_id: YouTube Video IDï¼ˆå¯é¸ï¼‰
        duration: å½±ç‰‡é•·åº¦ï¼ˆå¯é¸ï¼‰
    """
    channel: str
    title: str
    content: str
    published_at: str
    word_count: int
    file_path: Path
    video_id: str | None = None
    duration: str | None = None
    
    @property
    def content_preview(self, max_chars: int = 500) -> str:
        """å…§å®¹é è¦½ï¼ˆç”¨æ–¼ promptï¼‰"""
        if len(self.content) <= max_chars:
            return self.content
        return self.content[:max_chars] + "..."


@dataclass
class Segment:
    """
    å…§å®¹åˆ†æ®µï¼ˆç”¨æ–¼çµæ§‹åŒ–åˆ†æ®µï¼‰
    
    Attributes:
        section_type: æ®µè½é¡å‹ (intro, key_point, conclusion, etc.)
        title: æ®µè½æ¨™é¡Œ
        start_quote: éŒ¨é»æ–‡å­—ï¼ˆæ®µè½èµ·å§‹å¥ï¼Œç´„ 10-20 å­—ï¼‰
    """
    section_type: str
    title: str
    start_quote: str


@dataclass  
class AnalysisResult:
    """
    çµ±ä¸€çš„ LLM åˆ†æçµæœæ ¼å¼
    
    æ‰€æœ‰ Provider éƒ½å¿…é ˆå°‡è¼¸å‡ºè½‰æ›ç‚ºæ­¤æ¨™æº–æ ¼å¼ã€‚
    
    Attributes:
        semantic_summary: å…§å®¹æ‘˜è¦ï¼ˆ100-200 å­—ï¼‰
        key_topics: é—œéµä¸»é¡Œï¼ˆ3-5 å€‹ï¼‰
        suggested_topic: AI å»ºè­°çš„æ­¸æª”é¡åˆ¥ IDï¼ˆå°æ‡‰ topics.yaml ä¸­çš„ keyï¼‰
        content_type: å…§å®¹é¡å‹ (technical_analysis, opinion_discussion, news, ...)
        content_density: è³‡è¨Šå¯†åº¦ (high, medium, low)
        temporal_relevance: æ™‚æ•ˆæ€§ (evergreen, time_sensitive, news)
        dialogue_format: å°è©±å½¢å¼ï¼ˆå¯é¸ï¼‰
        segments: æ•˜äº‹çµæ§‹åˆ†æ®µï¼ˆå¯é¸ï¼Œç”¨æ–¼çµæ§‹åŒ–åˆ†æ®µï¼‰
        key_entities: é—œéµå¯¦é«”ï¼ˆå¯é¸ï¼Œå¦‚ [[Entity Name]]ï¼‰
        
        # ä¸­ç¹¼è³‡æ–™
        provider: ä½¿ç”¨çš„ Provider é¡å‹
        model: ä½¿ç”¨çš„æ¨¡å‹åç¨±
        processed_at: è™•ç†æ™‚é–“
    """
    semantic_summary: str
    key_topics: list[str]
    suggested_topic: str
    content_type: str  # technical_analysis, opinion_discussion, news, educational, interview
    content_density: str  # high, medium, low
    temporal_relevance: str  # evergreen, time_sensitive, news
    
    # å¯é¸æ¬„ä½
    dialogue_format: str | None = None
    segments: list[Segment] | None = None
    key_entities: list[str] | None = None
    
    # ä¸­ç¹¼è³‡æ–™ï¼ˆç”± Provider è‡ªå‹•å¡«å…¥ï¼‰
    provider: str = ""
    model: str = ""
    processed_at: datetime | None = None
    
    def __post_init__(self):
        if self.processed_at is None:
            self.processed_at = datetime.now()
    
    def to_dict(self) -> dict:
        """è½‰æ›ç‚ºå­—å…¸ï¼ˆç”¨æ–¼åºåˆ—åŒ–ï¼‰"""
        return {
            "semantic_summary": self.semantic_summary,
            "key_topics": self.key_topics,
            "suggested_topic": self.suggested_topic,
            "content_type": self.content_type,
            "content_density": self.content_density,
            "temporal_relevance": self.temporal_relevance,
            "dialogue_format": self.dialogue_format,
            "segments": [
                {"section_type": s.section_type, "title": s.title, "start_quote": s.start_quote}
                for s in (self.segments or [])
            ],
            "key_entities": self.key_entities or [],
            "analyzed_by": f"{self.provider}/{self.model}" if self.model else self.provider,
            "analyzed_at": self.processed_at.isoformat() if self.processed_at else None,
        }


# ============================================================================
# Provider Protocol å®šç¾©
# ============================================================================

@runtime_checkable
class LLMProvider(Protocol):
    """
    LLM Provider æŠ½è±¡ä»‹é¢
    
    æ‰€æœ‰ LLM å¯¦ä½œéƒ½å¿…é ˆéµå¾ªæ­¤ä»‹é¢ã€‚
    
    å¯¦ä½œç¯„ä¾‹:
        class GeminiCLIProvider:
            provider_type = ProviderType.GEMINI_CLI
            
            def analyze(self, input_data, prompt_template, output_path):
                # å¯¦ä½œå‘¼å«é‚è¼¯
                pass
            
            def health_check(self):
                # æª¢æŸ¥ gemini CLI æ˜¯å¦å¯ç”¨
                pass
    """
    
    provider_type: ProviderType
    
    def analyze(
        self,
        input_data: TranscriptInput,
        prompt_template: str,
        output_path: Path | None = None
    ) -> AnalysisResult:
        """
        åŸ·è¡Œèªæ„åˆ†æ
        
        Args:
            input_data: æ¨™æº–åŒ–çš„è½‰éŒ„è¼¸å…¥
            prompt_template: prompt æ¨¡æ¿åç¨±ï¼ˆå¦‚ "crypto_tech", "ufo_research"ï¼‰
            output_path: è¼¸å‡ºè¨˜éŒ„æª”è·¯å¾‘ï¼ˆä¾›é™¤éŒ¯/å¯©æŸ¥ï¼Œå¯é¸ï¼‰
                
                è‹¥æä¾›ï¼ŒProvider æ‡‰å°‡å®Œæ•´å°è©±è¨˜éŒ„å„²å­˜è‡³æ­¤è·¯å¾‘ï¼Œ
                ä½¿ç”¨ Markdown æ ¼å¼ï¼š
                ```
                # LLM å°è©±è¨˜éŒ„
                ## Prompt
                ...
                ## Response
                ...
                ```
        
        Returns:
            æ¨™æº–åŒ–çš„ AnalysisResult
        
        Raises:
            LLMCallError: å‘¼å«å¤±æ•—ï¼ˆå«éŒ¯èª¤ç¢¼ï¼‰
            LLMTimeoutError: å‘¼å«è¶…æ™‚
            LLMRateLimitError: é…é¡è€—ç›¡
        """
        ...
    
    def health_check(self) -> bool:
        """
        æª¢æŸ¥ Provider æ˜¯å¦å¯ç”¨
        
        Returns:
            True è¡¨ç¤ºå¯ç”¨ï¼ŒFalse è¡¨ç¤ºä¸å¯ç”¨
        """
        ...
    
    def get_model_info(self) -> dict:
        """
        å–å¾—æ¨¡å‹è³‡è¨Š
        
        Returns:
            {"name": str, "version": str, "capabilities": list[str]}
        """
        ...


# ============================================================================
# é€šç”¨ LLM å®¢æˆ¶ç«¯ä»‹é¢
# ============================================================================

class LLMClientInterface(Protocol):
    """
    é€šç”¨ LLM å®¢æˆ¶ç«¯ä»‹é¢
    
    å·¥å» æ¨¡å¼å¯¦ä½œï¼Œæ ¹æ“šé…ç½®å‹•æ…‹é¸æ“‡ Providerã€‚
    
    å¯¦ä½œç¯„ä¾‹:
        class LLMClient:
            def __init__(self, provider: LLMProvider):
                self._provider = provider
            
            @classmethod
            def from_config(cls, config):
                if config["provider"] == "gemini_cli":
                    provider = GeminiCLIProvider(...)
                return cls(provider)
    """
    
    def analyze(
        self,
        input_data: TranscriptInput,
        prompt_template: str = "default",
        output_path: Path | None = None
    ) -> AnalysisResult:
        """
        åŸ·è¡Œåˆ†æï¼ˆå§”æ´¾çµ¦åº•å±¤ Providerï¼‰
        
        Args:
            input_data: è½‰éŒ„è¼¸å…¥
            prompt_template: prompt æ¨¡æ¿åç¨±
            output_path: è¼¸å‡ºè¨˜éŒ„æª”è·¯å¾‘ï¼ˆå¯é¸ï¼‰
        
        Returns:
            AnalysisResult
        """
        ...
    
    def health_check(self) -> bool:
        """æª¢æŸ¥åº•å±¤ Provider æ˜¯å¦å¯ç”¨"""
        ...
    
    def get_provider_name(self) -> str:
        """å–å¾—ç›®å‰ä½¿ç”¨çš„ Provider åç¨±"""
        ...


# ============================================================================
# é…ç½®è³‡æ–™æ¨¡å‹
# ============================================================================

@dataclass
class GeminiCLIConfig:
    """Gemini CLI Provider é…ç½®"""
    project_dir: Path
    temp_dir: Path | None = None  # é è¨­ project_dir/temp
    timeout: int = 300
    max_retries: int = 3
    initial_retry_delay: int = 3


@dataclass
class OpenAIConfig:
    """OpenAI API Provider é…ç½®ï¼ˆé ç•™ï¼‰"""
    api_key: str
    model: str = "gpt-4"
    base_url: str | None = None  # ç”¨æ–¼è‡ªå®šç¾©ç«¯é»
    timeout: int = 60
    max_retries: int = 3


@dataclass
class LLMConfig:
    """é€šç”¨ LLM é…ç½®"""
    provider: ProviderType
    gemini_cli: GeminiCLIConfig | None = None
    openai: OpenAIConfig | None = None


# ============================================================================
# ä¾‹å¤–å®šç¾©
# ============================================================================

class LLMError(Exception):
    """LLM æ¨¡çµ„éŒ¯èª¤åŸºé¡"""
    pass


class LLMCallError(LLMError):
    """LLM å‘¼å«å¤±æ•—ï¼ˆéé›¶è¿”å›ç¢¼æˆ– API éŒ¯èª¤ï¼‰"""
    
    def __init__(self, message: str, exit_code: int | None = None, stderr: str = ""):
        super().__init__(message)
        self.exit_code = exit_code
        self.stderr = stderr


class LLMTimeoutError(LLMError):
    """LLM å‘¼å«è¶…æ™‚"""
    
    def __init__(self, message: str, timeout_seconds: int = 0):
        super().__init__(message)
        self.timeout_seconds = timeout_seconds


class LLMRateLimitError(LLMError):
    """LLM é…é¡è€—ç›¡æˆ–é€Ÿç‡é™åˆ¶"""
    
    def __init__(self, message: str, retry_after: int | None = None):
        super().__init__(message)
        self.retry_after = retry_after  # å»ºè­°ç­‰å¾…ç§’æ•¸


class LLMParseError(LLMError):
    """LLM è¼¸å‡ºè§£æå¤±æ•—"""
    pass


class PromptTemplateNotFoundError(LLMError):
    """Prompt æ¨¡æ¿ä¸å­˜åœ¨"""
    pass


# ============================================================================
# è¼”åŠ©é¡åˆ¥å”è­°
# ============================================================================

class PromptLoader(Protocol):
    """
    Prompt è¼‰å…¥å™¨ä»‹é¢
    
    å¾ prompts/{task_type}/{template}.md è¼‰å…¥ä¸¦æ ¼å¼åŒ– promptã€‚
    """
    
    def load(self, template_name: str, task_type: str = "analysis") -> str:
        """
        è¼‰å…¥ prompt template
        
        Args:
            template_name: Template åç¨±ï¼ˆå¦‚ "default", "crypto_tech"ï¼‰
            task_type: ä»»å‹™é¡å‹ï¼ˆé è¨­ "analysis"ï¼‰
        
        Returns:
            Template åŸå§‹å…§å®¹
        
        Raises:
            PromptTemplateNotFoundError: Template ä¸å­˜åœ¨
        """
        ...
    
    def format(
        self,
        template_name: str,
        input_data: TranscriptInput,
        task_type: str = "analysis"
    ) -> str:
        """
        è¼‰å…¥ä¸¦æ ¼å¼åŒ– prompt
        
        ä½¿ç”¨ Python str.format() æ›¿æ›è®Šæ•¸ï¼š
        - {channel} -> input_data.channel
        - {title} -> input_data.title
        - {file_path} -> æ²™ç›’å…§çš„ç›¸å°è·¯å¾‘
        - {word_count} -> input_data.word_count
        - {content_preview} -> input_data.content_preview
        
        Args:
            template_name: Template åç¨±
            input_data: è½‰éŒ„è¼¸å…¥
            task_type: ä»»å‹™é¡å‹
        
        Returns:
            å®Œæ•´çš„ prompt å­—ä¸²
        """
        ...


class OutputParser(Protocol):
    """
    LLM è¼¸å‡ºè§£æå™¨ä»‹é¢
    
    å¾åŸå§‹ LLM è¼¸å‡ºæå–çµæ§‹åŒ–è³‡æ–™ã€‚
    """
    
    def extract_response(self, output: str) -> str:
        """
        å¾å®Œæ•´è¼¸å‡ºæå– Response å€å¡Š
        
        æ”¯æ´æ ¼å¼ï¼š
        ```
        # Gemini Agent å°è©±è¨˜éŒ„
        ## Prompt
        ...
        ## Response
        {å¯¦éš›å›æ‡‰å…§å®¹}
        ```
        """
        ...
    
    def parse_analysis_result(self, response: str) -> AnalysisResult:
        """
        å°‡ Response è§£æç‚º AnalysisResult
        
        æ”¯æ´ JSON æˆ– YAML æ ¼å¼ã€‚
        
        Args:
            response: Response å€å¡Šå…§å®¹
        
        Returns:
            AnalysisResult
        
        Raises:
            LLMParseError: è§£æå¤±æ•—
        """
        ...


# ============================================================================
# å¯¦ä½œæŒ‡å°
# ============================================================================

"""
å¯¦ä½œæŒ‡å—èˆ‡æœ€ä½³å¯¦è¸

## 1. GeminiCLIProvider å¯¦ä½œç¯„æœ¬

```python
@dataclass
class GeminiCLIProvider:
    provider_type: ProviderType = ProviderType.GEMINI_CLI
    
    def analyze(self, input_data: TranscriptInput, prompt_template: str, 
                output_path: Path | None = None) -> AnalysisResult:
        # Step 1: æº–å‚™ transcript temp æª”æ¡ˆ
        with self._temp_transcript_file(input_data) as transcript_path:
            # Step 2: è¼‰å…¥ä¸¦æ ¼å¼åŒ– prompt
            prompt_content = self.prompt_loader.format(
                template_name=prompt_template,
                input_data=input_data,
                file_path=transcript_path.name  # é—œéµï¼šåªçµ¦æª”åï¼
            )
            
            # Step 3: å°‡ prompt å¯«å…¥ temp æª”æ¡ˆï¼ˆé¿å… shell è½‰ç¾©å•é¡Œï¼‰
            prompt_path = self._write_prompt_file(prompt_content, input_data)
            
            try:
                # Step 4: ä½¿ç”¨ç°¡çŸ­çš„ meta prompt åŸ·è¡Œ Gemini
                meta_prompt = (
                    f"è«‹è®€å– {prompt_path.name} ä¸¦æŒ‰ç…§å…¶ä¸­æŒ‡ç¤ºåˆ†æ "
                    f"{transcript_path.name}ï¼Œç„¶å¾Œè¼¸å‡º JSON çµæœ"
                )
                raw_output = self._call_gemini_with_retry(meta_prompt)
                
                # Step 5: è¨˜éŒ„å°è©±ï¼ˆå¯é¸ï¼‰
                if output_path:
                    self._save_conversation(prompt_content, raw_output, output_path)
                
                # Step 6: è§£æçµæœ
                analysis_result = self.output_parser.parse_analysis_result(raw_output)
                analysis_result.provider = self.provider_type.value
                analysis_result.model = "gemini-2.0-flash"
                
                return analysis_result
                
            finally:
                # Step 7: æ¸…ç† prompt temp æª”æ¡ˆ
                self._cleanup_temp_file(prompt_path)
```

## 2. å…©æª”æ¡ˆå‚³éæ©Ÿåˆ¶ï¼ˆPrompt + Transcriptï¼‰

ç‚ºé¿å… shell ç‰¹æ®Šå­—å…ƒè½‰ç¾©å•é¡Œï¼Œä½¿ç”¨å…©å€‹ç¨ç«‹æª”æ¡ˆå‚³éçµ¦ Geminiï¼š

```python
def _write_prompt_file(self, prompt_content: str, input_data: TranscriptInput) -> Path:
    '''
    å°‡ prompt å…§å®¹å¯«å…¥ temp æª”æ¡ˆ
    
    Args:
        prompt_content: æ ¼å¼åŒ–å¾Œçš„å®Œæ•´ prompt å…§å®¹
        input_data: è½‰éŒ„è¼¸å…¥ï¼ˆç”¨æ–¼ç”¢ç”Ÿå”¯ä¸€æª”åï¼‰
        
    Returns:
        Prompt æª”æ¡ˆè·¯å¾‘ï¼ˆä½æ–¼ project_dir/temp/ ä¸‹ï¼‰
    '''
    content_hash = hash(prompt_content) % 10000
    temp_name = f"prompt_task_{input_data.channel}_{content_hash}.md"
    temp_path = self.temp_dir / temp_name
    
    temp_path.write_text(prompt_content, encoding="utf-8")
    return temp_path

def _call_gemini_with_retry(self, meta_prompt: str) -> str:
    '''
    åŸ·è¡Œ Gemini CLIï¼ˆå«æŒ‡æ•¸é€€é¿é‡è©¦ï¼‰
    
    é‡è¦æ”¹é€²ï¼š
    - ä¸å†å°‡å®Œæ•´ prompt å‚³å…¥ shell åƒæ•¸
    - åªå‚³éç°¡çŸ­çš„ meta promptï¼ˆå¼•ç”¨ temp æª”æ¡ˆåç¨±ï¼‰
    - Gemini æœƒå…ˆè®€å– prompt_task_xxx.mdï¼Œå†ä¾ç…§æŒ‡ç¤ºè®€å– transcript_xxx.md
    
    Args:
        meta_prompt: ç°¡çŸ­çš„ meta promptï¼ˆå¦‚ã€Œè«‹è®€å– prompt_task_xxx.md...ã€ï¼‰
    '''
    for attempt in range(1, self.max_retries + 1):
        try:
            result = subprocess.run(
                [
                    "gemini",
                    "-p", meta_prompt,           # ç°¡çŸ­ï¼Œç„¡ç‰¹æ®Šå­—å…ƒé¢¨éšª
                    "-o", "json",                # JSON è¼¸å‡º
                    "--approval-mode", "plan"    # å”¯è®€æ¨¡å¼
                ],
                capture_output=True,
                text=True,
                timeout=self.timeout,
                cwd=str(self.project_dir)  # é—œéµï¼šåœ¨ project_dir åŸ·è¡Œï¼
            )
            
            if result.returncode == 0:
                return result.stdout
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºé…é¡è€—ç›¡
            if "exhausted your capacity" in result.stderr:
                if attempt < self.max_retries:
                    delay = min(3 * (2 ** (attempt - 1)), 30)
                    time.sleep(delay)
                    continue
            
            raise LLMCallError(
                f"Gemini failed: {result.stderr}",
                exit_code=result.returncode,
                stderr=result.stderr
            )
            
        except subprocess.TimeoutExpired:
            if attempt == self.max_retries:
                raise LLMTimeoutError(
                    f"Timeout after {self.timeout}s",
                    timeout_seconds=self.timeout
                )

def _cleanup_temp_file(self, temp_path: Path) -> None:
    '''æ¸…ç†è‡¨æ™‚æª”æ¡ˆï¼ˆç„¡è«–æˆåŠŸå¤±æ•—éƒ½åŸ·è¡Œï¼‰'''
    try:
        if temp_path.exists():
            temp_path.unlink()
    except OSError:
        pass
```

### ç‚ºä»€éº¼ä½¿ç”¨å…©å€‹æª”æ¡ˆï¼Ÿ

| æ–¹æ¡ˆ | å„ªé» | ç¼ºé» |
|-----|------|------|
| **å–®ä¸€æª”æ¡ˆï¼ˆåˆä½µï¼‰** | Gemini åªè®€ä¸€æ¬¡ | éœ€è¦ä¿®æ”¹ prompt æ¨¡æ¿ï¼›åˆ†éš”ç·šå¯èƒ½èˆ‡å…§å®¹è¡çª |
| **å…©å€‹æª”æ¡ˆï¼ˆæ¡ç”¨ï¼‰** | è·è²¬åˆ†é›¢ï¼›prompt æ¨¡æ¿ç„¡éœ€ä¿®æ”¹ï¼›é™¤éŒ¯æ™‚å¯å–®ç¨æŸ¥çœ‹ | Gemini è®€å…©æ¬¡ï¼ˆå¯å¿½ç•¥ï¼‰ |

**temp/ ç›®éŒ„çµæ§‹ï¼š**
```
temp/
â”œâ”€â”€ prompt_task_Bankless_7842.md   â† å®Œæ•´ prompt ä»»å‹™èªªæ˜ï¼ˆ4KBï¼‰
â””â”€â”€ transcript_Bankless_7842.md    â† è½‰éŒ„ç¨¿å…§å®¹ï¼ˆ100KB+ï¼‰
```

**Shell æŒ‡ä»¤ï¼š**
```bash
gemini -p "è«‹è®€å– prompt_task_Bankless_7842.md ä¸¦æŒ‰ç…§å…¶ä¸­æŒ‡ç¤ºåˆ†æ transcript_Bankless_7842.mdï¼Œç„¶å¾Œè¼¸å‡º JSON çµæœ" \
       -o json \
       --approval-mode plan
```

## 3. ä½¿ç”¨ Context Manager å„ªé›…è™•ç†

```python
from contextlib import contextmanager

@contextmanager
def temp_transcript_file(project_dir: Path, input_data: TranscriptInput):
    '''
    ä½¿ç”¨ context manager ç¢ºä¿ temp æª”æ¡ˆä¸€å®šè¢«æ¸…ç†
    
    ä½¿ç”¨ç¯„ä¾‹:
        with temp_transcript_file(project_dir, input_data) as temp_path:
            result = provider.analyze(..., file_path=temp_path.name)
    '''
    temp_dir = project_dir / "temp"
    temp_dir.mkdir(exist_ok=True)
    
    temp_path = temp_dir / f"{input_data.channel}_{hash(input_data.content[:100])}.md"
    temp_path.write_text(input_data.content, encoding="utf-8")
    
    try:
        yield temp_path
    finally:
        try:
            temp_path.unlink(missing_ok=True)
        except OSError:
            pass

# ä½¿ç”¨ç¯„ä¾‹
class GeminiCLIProvider:
    def analyze(self, input_data: TranscriptInput, ...):
        with temp_transcript_file(self.project_dir, input_data) as temp_path:
            prompt = self.prompt_loader.format(
                template_name=prompt_template,
                input_data=input_data,
                file_path=temp_path.name
            )
            raw_output = self._call_gemini(prompt)
            return self.output_parser.parse_analysis_result(raw_output)
```

## 4. LLMClient å·¥å» æ¨¡å¼å¯¦ä½œ

```python
class LLMClient:
    '''é€šç”¨ LLM å®¢æˆ¶ç«¯ï¼ˆå·¥å» æ¨¡å¼ï¼‰'''
    
    def __init__(self, provider: LLMProvider):
        self._provider = provider
    
    @classmethod
    def from_config(cls, config: dict) -> "LLMClient":
        '''
        æ ¹æ“šé…ç½®å»ºç«‹å°æ‡‰çš„ Provider
        
        Args:
            config: {
                "provider": "gemini_cli",
                "project_dir": "/path/to/project",
                "timeout": 300,
                ...
            }
        '''
        provider_type = ProviderType(config.get("provider", "gemini_cli"))
        
        if provider_type == ProviderType.GEMINI_CLI:
            provider = GeminiCLIProvider(
                project_dir=Path(config["project_dir"]),
                timeout=config.get("timeout", 300),
                max_retries=config.get("max_retries", 3)
            )
        elif provider_type == ProviderType.OPENAI_API:
            raise NotImplementedError("OpenAI API provider å°šæœªå¯¦ä½œ")
        else:
            raise ValueError(f"æœªçŸ¥çš„ provider: {provider_type}")
        
        return cls(provider)
    
    def analyze(self, input_data: TranscriptInput, 
                prompt_template: str = "default",
                output_path: Path | None = None) -> AnalysisResult:
        '''å§”æ´¾çµ¦åº•å±¤ Provider'''
        return self._provider.analyze(input_data, prompt_template, output_path)
```

## 5. éŒ¯èª¤è™•ç†æ¨¡å¼

```python
# æ¨è–¦çš„éŒ¯èª¤è™•ç†æ¨¡å¼
try:
    result = llm_client.analyze(input_data, prompt_template="crypto_tech")
except LLMRateLimitError as e:
    # é…é¡è€—ç›¡ï¼Œè¨˜éŒ„ä¸¦æ¨™è¨˜ç‚ºç¨å¾Œé‡è©¦
    logger.warning(f"Rate limit hit, retry after {e.retry_after}s")
    mark_for_retry(file, delay=e.retry_after)
except LLMTimeoutError as e:
    # è¶…æ™‚ï¼Œè¨˜éŒ„ä¸¦æ¨™è¨˜ç‚ºå¤±æ•—
    logger.error(f"LLM timeout after {e.timeout_seconds}s")
    mark_as_failed(file, error_code="LLM_TIMEOUT")
except LLMCallError as e:
    # å…¶ä»–éŒ¯èª¤ï¼Œå€åˆ†æ˜¯å¦å¯é‡è©¦
    if e.exit_code in [5, 6]:  # å‡è¨­ 5xx éŒ¯èª¤å¯é‡è©¦
        mark_for_retry(file)
    else:
        mark_as_failed(file, error_code="LLM_ERROR", details=e.stderr)
except LLMParseError as e:
    # è§£æå¤±æ•—ï¼Œé€šå¸¸ä¸é‡è©¦ï¼ˆprompt æˆ–è¼¸å‡ºæ ¼å¼å•é¡Œï¼‰
    logger.error(f"Failed to parse LLM output: {e}")
    mark_as_failed(file, error_code="PARSE_ERROR")
```

## 6. å¸¸è¦‹é™·é˜±

| é™·é˜± | éŒ¯èª¤ç¤ºç¯„ | æ­£ç¢ºåšæ³• |
|------|----------|----------|
| **ç›´æ¥å‚³éé•·å…§å®¹åˆ° shell** | `subprocess.run(["gemini", "-p", long_prompt])` | å¯«å…¥ temp æª”æ¡ˆï¼Œå‚³éç°¡çŸ­å¼•ç”¨ |
| ç›´æ¥å‚³éå…§å®¹ | `subprocess.run(["gemini", content[:50000]])` | å¯«å…¥ temp æª”æ¡ˆï¼Œå‚³éè·¯å¾‘ |
| å¿½ç•¥ cwd | `subprocess.run([...])` é è¨­ cwd | æ˜ç¢ºæŒ‡å®š `cwd=str(project_dir)` |
| å¿˜è¨˜æ¸…ç† | æ²’æœ‰ try/finally | ä½¿ç”¨ context manager æˆ– try/finally |
| éŒ¯èª¤è™•ç†ä¸å®Œæ•´ | åª catch Exception | å€åˆ† LLMRateLimitErrorã€LLMTimeoutError |
| æª”åè¡çª | å›ºå®šæª”å `temp/input.md` | ä½¿ç”¨ hash ç”¢ç”Ÿå”¯ä¸€æª”å |

### âš ï¸ Shell è½‰ç¾©é¢¨éšªï¼ˆé‡è¦ï¼ï¼‰

**çµ•å°ä¸è¦å°‡å®Œæ•´ prompt ç›´æ¥å‚³å…¥ shell åƒæ•¸ï¼š**

```python
# âŒ éŒ¯èª¤ï¼šprompt ä¸­çš„åå¼•è™Ÿã€å¼•è™Ÿã€æ›è¡Œå¯èƒ½ç ´å£ shell å‘½ä»¤
subprocess.run(
    ["gemini", "-p", prompt],  # prompt å¯èƒ½åŒ…å« `code`ã€"quotes"ã€\n
# âœ… æ­£ç¢ºï¼šä½¿ç”¨å…©å€‹ temp æª”æ¡ˆï¼Œshell åªå‚³ç°¡çŸ­å¼•ç”¨
subprocess.run(
    ["gemini", "-p", "è«‹è®€å– prompt_task_xxx.md ä¸¦æŒ‰ç…§å…¶ä¸­æŒ‡ç¤ºåˆ†æ transcript_xxx.md"],
```

**åŸå› ï¼š**
- Markdown ä¸­çš„åå¼•è™Ÿ `` ` `` åœ¨ shell ä¸­æœ‰ç‰¹æ®Šæ„ç¾©
- å¤šè¡Œå­—ä¸²å¯èƒ½å°è‡´åƒæ•¸è§£æéŒ¯èª¤
- é›£ä»¥é æ¸¬çš„ç‰¹æ®Šå­—å…ƒçµ„åˆ

**è§£æ±ºæ–¹æ¡ˆï¼š**
1. å°‡å®Œæ•´ prompt å¯«å…¥ `temp/prompt_task_{hash}.md`
2. Shell åƒæ•¸åªå‚³éç°¡çŸ­çš„ meta promptï¼ˆå¼•ç”¨æª”æ¡ˆåç¨±ï¼‰
3. Gemini æœƒä¾ç…§ meta prompt çš„æŒ‡ç¤ºè®€å–ä¸¦åŸ·è¡Œä»»å‹™

## 7. Gemini CLI é¸é …åƒè€ƒ

> ğŸ’¡ **æç¤º**ï¼šæœ¬ç¯€æ•´ç†äº†èˆ‡æœ¬å°ˆæ¡ˆç›¸é—œçš„å¸¸ç”¨é¸é …ã€‚è‹¥è¦æŸ¥çœ‹å®Œæ•´å‘½ä»¤èªªæ˜ï¼Œè«‹åœ¨çµ‚ç«¯æ©ŸåŸ·è¡Œï¼š
> ```bash
> gemini --help
> ```

æ ¹æ“š `gemini --help`ï¼Œä»¥ä¸‹é¸é …èˆ‡æœ¬å°ˆæ¡ˆç›¸é—œï¼š

### é—œéµé¸é …

| é¸é … | èªªæ˜ | å»ºè­° |
|------|------|------|
| `-p, --prompt` | **å¿…é ˆä½¿ç”¨ï¼** å•Ÿå‹• non-interactive (headless) æ¨¡å¼ | çµ•å°å¿…è¦ï¼Œå¦å‰‡é€²å…¥äº’å‹•æ¨¡å¼ç„¡æ³•æ“·å–è¼¸å‡º |
| `-m, --model` | æŒ‡å®šæ¨¡å‹ | `gemini-2.5-pro` |
| `-o, --output-format` | è¼¸å‡ºæ ¼å¼ï¼š`text`, `json`, `stream-json` | å»ºè­°ä½¿ç”¨ `json` |
| `-s, --sandbox` | å•Ÿç”¨æ²™ç›’æ¨¡å¼ | é è¨­å·²å•Ÿç”¨ |
| `--approval-mode` | æ ¸å‡†æ¨¡å¼ï¼š`default`, `auto_edit`, `yolo`, `plan` | **ä½¿ç”¨ `plan`**ï¼ˆå”¯è®€ï¼Œæœ€å®‰å…¨ï¼‰ |

### å¯¦ä½œå»ºè­°

**æœ€ä½³å‘½ä»¤çµ„åˆ**ï¼ˆheadlessã€JSON è¼¸å‡ºã€plan æ¨¡å¼ï¼‰ï¼š

```python
result = subprocess.run(
    [
        "gemini",
        "-p", prompt,                    # headless æ¨¡å¼ï¼ˆå¿…è¦ï¼‰
        "-o", "json",                    # JSON è¼¸å‡ºï¼ˆä¾¿æ–¼è§£æï¼‰
        "-m", "gemini-2.5-pro",          # æŒ‡å®šæ¨¡å‹
        "--approval-mode", "plan"        # plan æ¨¡å¼ï¼ˆå”¯è®€ï¼Œæœ€å®‰å…¨ï¼‰
    ],
    capture_output=True,
    text=True,
    timeout=120,                         # çµ¦è¼ƒé•· timeoutï¼ˆå†·å•Ÿå‹•ï¼‰
    cwd=str(self.project_dir)
)

# æå– JSON å€å¡Šï¼ˆè™•ç†å¯èƒ½çš„é›œè¨Šï¼‰
json_data = extract_json_block(result.stdout)
```

### è¼¸å‡ºè§£æï¼šæå– JSON å€å¡Š

Gemini CLI çš„è¼¸å‡ºå¯èƒ½åŒ…å«é›œè¨Šï¼ˆå¦‚åˆå§‹åŒ–æ—¥èªŒã€æ€è€ƒéç¨‹ï¼‰ï¼Œéœ€è¦ç©©å¥åœ°æå– JSONï¼š

```python
import json
import re

def extract_json_block(text: str) -> dict:
    '''
    å¾ Gemini è¼¸å‡ºä¸­æå– JSON å€å¡Š
    
    è™•ç†å ´æ™¯ï¼š
    1. ç´” JSON è¼¸å‡º
    2. JSON å‰å¾Œæœ‰é›œè¨Šæ–‡å­—
    3. æ€è€ƒéç¨‹ + JSON å›è¦†
    4. å¤šå€‹ JSON å€å¡Šï¼ˆå–æœ€å¾Œä¸€å€‹ï¼‰
    
    Args:
        text: Gemini CLI çš„åŸå§‹è¼¸å‡º
    
    Returns:
        è§£æå¾Œçš„ dict
    
    Raises:
        LLMParseError: æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ JSON
    '''
    # ç­–ç•¥ 1: å°‹æ‰¾ ```json ... ``` ä»£ç¢¼å¡Š
    json_block_pattern = r'```json\s*(.*?)\s*```'
    matches = re.findall(json_block_pattern, text, re.DOTALL)
    if matches:
        try:
            return json.loads(matches[-1])  # å–æœ€å¾Œä¸€å€‹
        except json.JSONDecodeError:
            pass
    
    # ç­–ç•¥ 2: å°‹æ‰¾ ``` ... ```ï¼ˆç„¡èªè¨€æ¨™è¨˜ï¼‰
    code_block_pattern = r'```\s*(\{.*?\})\s*```'
    matches = re.findall(code_block_pattern, text, re.DOTALL)
    if matches:
        try:
            return json.loads(matches[-1])
        except json.JSONDecodeError:
            pass
    
    # ç­–ç•¥ 3: å°‹æ‰¾ { ... } å€å¡Šï¼ˆæœ€å¯¬é¬†ï¼‰
    # å¾å¾Œå¾€å‰æ‰¾ï¼Œæ‰¾åˆ°ç¬¬ä¸€å€‹å®Œæ•´çš„ JSON object
    brace_count = 0
    start_idx = None
    
    for i, char in enumerate(reversed(text)):
        if char == '}':
            if brace_count == 0:
                end_idx = len(text) - i
            brace_count += 1
        elif char == '{':
            brace_count -= 1
            if brace_count == 0 and start_idx is None:
                start_idx = len(text) - i - 1
                break
    
    if start_idx is not None and end_idx is not None:
        try:
            return json.loads(text[start_idx:end_idx])
        except json.JSONDecodeError:
            pass
    
    # ç­–ç•¥ 4: å˜—è©¦ç›´æ¥è§£ææ•´å€‹è¼¸å‡ºï¼ˆç´” JSONï¼‰
    try:
        return json.loads(text.strip())
    except json.JSONDecodeError:
        raise LLMParseError(f"ç„¡æ³•å¾è¼¸å‡ºä¸­æå–æœ‰æ•ˆçš„ JSON: {text[:200]}...")
```

### Prompt è¨­è¨ˆå»ºè­°

ç‚ºäº†æœ€å¤§åŒ– JSON è¼¸å‡ºçš„ç©©å®šæ€§ï¼Œå»ºè­°åœ¨ prompt ä¸­æ˜ç¢ºè¦æ±‚ï¼š

```markdown
# prompts/analysis/default.md

ä½ æ˜¯ä¸€å€‹å…§å®¹åˆ†æå¸«ã€‚è«‹åˆ†ææª”æ¡ˆ {file_path} çš„å…§å®¹ï¼Œä¸¦ä»¥ JSON æ ¼å¼å›è¦†ã€‚

è«‹åš´æ ¼éµå®ˆä»¥ä¸‹æ ¼å¼ï¼Œä¸è¦åŠ å…¥ä»»ä½•å…¶ä»–æ–‡å­—æˆ–è§£é‡‹ï¼š

```json
{
  "semantic_summary": "100-200å­—çš„å…§å®¹æ‘˜è¦",
  "key_topics": ["ä¸»é¡Œ1", "ä¸»é¡Œ2", "ä¸»é¡Œ3"],
  "content_type": "technical_analysis",
  "content_density": "high",
  "temporal_relevance": "evergreen"
}
```

é‡è¦ï¼š
1. åªå›è¦† JSONï¼Œä¸è¦æœ‰ä»»ä½•å‰è¨€æˆ–çµèª
2. ç¢ºä¿ JSON æ ¼å¼æ­£ç¢ºï¼ˆç„¡ trailing commasï¼‰
3. æ‰€æœ‰å­—ä¸²ä½¿ç”¨é›™å¼•è™Ÿ
```

### å†·å•Ÿå‹•æ™‚é–“è€ƒé‡

å¯¦æ¸¬ç™¼ç¾ Gemini CLI æœ‰æ˜é¡¯çš„å†·å•Ÿå‹•æ™‚é–“ï¼š
- é¦–æ¬¡å‘¼å«ï¼šç´„ 5-10 ç§’ï¼ˆè¼‰å…¥æ†‘è­‰ã€åˆå§‹åŒ–æœå‹™ï¼‰
- è¼¸å‡ºæ ¼å¼ï¼šåˆå§‹åŒ–æ—¥èªŒ + å¯¦éš›å›è¦†

**å»ºè­°**ï¼š
- `timeout` è¨­ç‚º 120 ç§’ï¼ˆè€Œé 60 ç§’ï¼‰
- æ‰¹æ¬¡è™•ç†æ™‚è€ƒæ…®æ­¤å»¶é²

### Approval Mode é¸æ“‡

| æ¨¡å¼ | èªªæ˜ | é¢¨éšª | å»ºè­° |
|------|------|------|------|
| `plan` | å”¯è®€æ¨¡å¼ï¼Œä¸å…è¨±ä»»ä½•ä¿®æ”¹ | æœ€ä½ | âœ… **æ¡ç”¨** |
| `auto_edit` | è‡ªå‹•æ ¸å‡†ç·¨è¼¯å‹•ä½œ | ä¸­ | å¯é¸ |
| `yolo` | è‡ªå‹•æ ¸å‡†æ‰€æœ‰å‹•ä½œ | é«˜ | ä¸å»ºè­° |
| `default` | æ¯æ¬¡ç¢ºèª | æœƒå¡ä½ | ä¸é©åˆè‡ªå‹•åŒ– |

**æ¡ç”¨ `plan` æ¨¡å¼çš„åŸå› **ï¼š
- æˆ‘å€‘åªéœ€è¦ Gemini **è®€å–** temp/ æª”æ¡ˆä¸¦**å›å‚³åˆ†æçµæœ**
- ä¸éœ€è¦ Gemini åŸ·è¡Œä»»ä½•æª”æ¡ˆä¿®æ”¹
- å³ä½¿ prompt è¢«æ³¨å…¥æƒ¡æ„æŒ‡ä»¤ï¼Œ`plan` æ¨¡å¼ä¹Ÿæœƒé˜»æ­¢åŸ·è¡Œ

"""

# ============================================================================
# é©—æ”¶æ¨™æº–
# ============================================================================

"""
é©—æ”¶æ¸¬è©¦é …ç›®ï¼š

1. TranscriptInput
   - æ­£ç¢ºå»ºç«‹èˆ‡å±¬æ€§å­˜å–
   - content_preview æ­£ç¢ºæˆªæ–·

2. AnalysisResult
   - æ­£ç¢ºè½‰æ›ç‚ºå­—å…¸
   - processed_at è‡ªå‹•å¡«å…¥
   - æ”¯æ´å¯é¸æ¬„ä½

3. LLMProvider Protocol
   - GeminiCLIProvider ç¬¦åˆå”è­°
   - æœªä¾† OpenAIProvider ç¬¦åˆå”è­°

4. LLMClient
   - from_config æ­£ç¢ºå»ºç«‹å°æ‡‰ Provider
   - analyze å§”æ´¾çµ¦åº•å±¤ Provider
   - health_check æ­£å¸¸é‹ä½œ

5. ä¾‹å¤–è™•ç†
   - LLMCallErrorã€LLMTimeoutErrorã€LLMRateLimitError æ­£ç¢ºæ‹‹å‡ºèˆ‡æ•ç²

6. PromptLoader
   - æ­£ç¢ºè¼‰å…¥ prompts/analysis/{template}.md
   - æ­£ç¢ºæ›¿æ›è®Šæ•¸
   - Template ä¸å­˜åœ¨æ™‚æ‹‹å‡º PromptTemplateNotFoundError

7. OutputParser
   - æ­£ç¢ºæå– Response å€å¡Š
   - æ­£ç¢ºè§£æ JSON/YAML
   - æ­£ç¢ºè½‰æ›ç‚º AnalysisResult

åŸ·è¡Œé©—æ”¶æ¸¬è©¦ï¼š
    python docs/interfaces/tests/test_llm.py
"""

#!/usr/bin/env python3
"""
Integration Test: Gemini CLI stdin Optimization

æ­¤æ•´åˆæ¸¬è©¦é©—è­‰ stdin å„ªåŒ–å¾Œçš„ Gemini CLI Provider æ˜¯å¦æ­£å¸¸é‹ä½œï¼š
1. é€é stdin å‚³é prompt + transcriptï¼ˆ1 æ¬¡ API å‘¼å«ï¼Œè€Œé 3-4 æ¬¡ï¼‰
2. è¨˜éŒ„é™¤éŒ¯è¼¸å…¥åˆ° temp/debug/ï¼ˆä¾¿æ–¼è§€æ¸¬å¯¦éš›å‚³éå…§å®¹ï¼‰
3. è§£æçµæœä¸¦å„²å­˜åˆ° intermediate/pending/

åŸ·è¡Œæ–¹å¼:
    cd /home/openclaw/Projects/knowledge-pipeline
    source venv/bin/activate
    python tests/integration/test_gemini_stdin_optimization.py

æ³¨æ„:
    - æ­¤æ¸¬è©¦æœƒå¯¦éš›å‘¼å« Gemini APIï¼Œè«‹ç¢ºèªé¡åº¦å……è¶³
    - æ¸¬è©¦æœƒä½¿ç”¨ config/config.yaml çš„è¨­å®š
    - è¼¸å‡ºæª”æ¡ˆæœƒä¿å­˜åœ¨ temp/debug/ å’Œ intermediate/pending/
"""

import sys
from pathlib import Path

# ç¢ºä¿èƒ½æ‰¾åˆ° src æ¨¡çµ„ï¼ˆå¾ tests/integration/ å›åˆ°å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼‰
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.config import ConfigLoader
from src.discovery import DiscoveryService
from src.llm import LLMClient
from src.analyzer import AnalyzerService


def main():
    print("=" * 60)
    print("æ¸¬è©¦ stdin å„ªåŒ–å¾Œçš„ Gemini CLI Provider")
    print("=" * 60)
    
    # è¼‰å…¥è¨­å®š
    config_loader = ConfigLoader()
    config = config_loader.load_pipeline_config()
    print(f"\nâœ… è¨­å®šè¼‰å…¥å®Œæˆ")
    print(f"   - Provider: {config.llm.provider}")
    print(f"   - Project Dir: {config.llm.project_dir}")
    print(f"   - Timeout: {config.llm.timeout}s")
    print(f"   - Max Retries: {config.llm.max_retries}")
    
    # åˆå§‹åŒ– Discovery Service
    discovery = DiscoveryService()
    
    # æƒæä¸€å€‹æª”æ¡ˆä¾†æ¸¬è©¦
    print(f"\nğŸ“ æƒæè½‰éŒ„æª”æ¡ˆ...")
    print(f"   - è¼¸å…¥ç›®éŒ„: {config.transcriber_output}")
    
    transcripts = discovery.discover(
        root_dir=config.transcriber_output,
        min_word_count=100
    )
    
    if not transcripts:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•è½‰éŒ„æª”æ¡ˆ")
        return 1
    
    # åªå–ç¬¬ä¸€å€‹æª”æ¡ˆæ¸¬è©¦
    transcripts = transcripts[:1]
    
    transcript = transcripts[0]
    print(f"âœ… æ‰¾åˆ°æ¸¬è©¦æª”æ¡ˆ:")
    print(f"   - Channel: {transcript.metadata.channel}")
    print(f"   - Title: {transcript.metadata.title[:50]}...")
    print(f"   - Word Count: {transcript.metadata.word_count}")
    print(f"   - Path: {transcript.path}")
    
    # åˆå§‹åŒ– LLM Client
    print(f"\nğŸ¤– åˆå§‹åŒ– LLM Client...")
    llm_config = {
        "provider": config.llm.provider,
        "project_dir": str(config.llm.project_dir),
        "timeout": config.llm.timeout,
        "max_retries": config.llm.max_retries,
        "debug_input": True  # é–‹å•Ÿé™¤éŒ¢æ¨¡å¼
    }
    
    client = LLMClient.from_config(llm_config)
    print(f"âœ… LLM Client åˆå§‹åŒ–å®Œæˆ")
    print(f"   - Provider: {client.get_provider_name()}")
    
    # åˆå§‹åŒ– Analyzer
    analyzer = AnalyzerService(
        llm_client=client,
        enable_segmentation=True,
        default_template="default"
    )
    
    # ç¢ºå®šè¼¸å‡ºç›®éŒ„
    output_dir = config.intermediate / "pending"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åŸ·è¡Œåˆ†æ
    print(f"\nğŸ“ é–‹å§‹åˆ†æï¼ˆé€é stdin å‚³éå…§å®¹ï¼‰...")
    print(f"   - Template: default")
    print(f"   - Output: {output_dir}")
    print(f"   - é æœŸ API å‘¼å«æ¬¡æ•¸: 1 æ¬¡ï¼ˆå„ªåŒ–å¾Œï¼‰")
    print(f"   - æ³¨æ„ï¼šé€™æœƒå¯¦éš›å‘¼å« Gemini APIï¼Œè«‹ç¢ºèªé¡åº¦è¶³å¤ ")
    print()
    
    try:
        result = analyzer.analyze(
            transcript=transcript,
            prompt_template="default",
            output_dir=output_dir
        )
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        print(f"   - Semantic Summary: {result.analysis.semantic_summary[:100]}...")
        print(f"   - Key Topics: {result.analysis.key_topics}")
        print(f"   - Content Type: {result.analysis.content_type}")
        print(f"   - Content Density: {result.analysis.content_density}")
        print(f"   - Temporal Relevance: {result.analysis.temporal_relevance}")
        
        # æª¢æŸ¥é™¤éŒ¯æª”æ¡ˆ
        debug_dir = Path("temp/debug")
        if debug_dir.exists():
            debug_files = list(debug_dir.glob("debug_*.md"))
            if debug_files:
                # æ‰¾æœ€æ–°çš„æª”æ¡ˆ
                latest = max(debug_files, key=lambda p: p.stat().st_mtime)
                print(f"\nğŸ“„ é™¤éŒ¢æª”æ¡ˆå·²ç”Ÿæˆ:")
                print(f"   - {latest}")
                print(f"   - å¤§å°: {latest.stat().st_size} bytes")
                print(f"\n   å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤æ‰‹å‹•æ¸¬è©¦:")
                print(f"   cat '{latest}' | gemini -p \"Analyze and output JSON\"")
        
        print(f"\nğŸ’¾ è¼¸å‡ºæª”æ¡ˆä½ç½®:")
        expected_path = output_dir / transcript.metadata.channel / transcript.metadata.published_at.strftime("%Y-%m")
        print(f"   - {expected_path}")
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
